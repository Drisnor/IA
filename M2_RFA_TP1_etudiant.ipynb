{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M2_RFA_TP1_etudiant.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Drisnor/IA/blob/master/M2_RFA_TP1_etudiant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEKUYgqIqZtF",
        "colab_type": "text"
      },
      "source": [
        "# M2 IARF-RODECO TP 1 : classification et régression avec un MLP/FCNN\n",
        "\n",
        "MLP : Multi-Layer Perception ou Perceptron Multi-Couche en français\n",
        "\n",
        "Un MLP peut s'appeler également FCNN pour Fully-Connected Neural Network\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSDXptnTxcab",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**Auteurs :** Benjamin Chamand - Thomas Pellegrini\n",
        "\n",
        "**Contributeur :** Lionel Pibre (sept 2020)\n",
        "\n",
        "**Année de création :** 2018"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8J_fmZVxxiOc",
        "colab_type": "text"
      },
      "source": [
        "## Gestion des pré-requis avant de commencer le TP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjNqfY-5rJ5O",
        "colab_type": "text"
      },
      "source": [
        "### Importation des bibliothèques\n",
        "\n",
        "Premièrement, il faut faire les import nécessaires au TP :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIp5u7_H8JTB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.utils import np_utils\n",
        "from keras import backend as K\n",
        "from keras import activations\n",
        "\n",
        "from distutils.version import LooseVersion as LV\n",
        "from keras import __version__\n",
        "\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "import pandas as pd\n",
        "\n",
        "print('Using Keras version:', __version__, 'backend:', K.backend())\n",
        "assert(LV(__version__) >= LV(\"2.0.0\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PghD6wNFrXxO",
        "colab_type": "text"
      },
      "source": [
        "Si on utilise le backend Tensorflow (ce qui est le cas par défaut) on peut facilement avoir des informations sur le CPU ou le GPU accessible.\n",
        "\n",
        "Dans un premier temps, vérifier bien que keras utilise le GPU pour les calculs.\n",
        "Si ce n'est pas le cas, il faut aller dans Exécution > Modifier le type d'exécution > Accélérateur Matériel > GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqeQm-bMs8dY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if K.backend() == \"tensorflow\":\n",
        "    from tensorflow.python.client import device_lib\n",
        "    print(device_lib.list_local_devices())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pelBlG6Nv0CR",
        "colab_type": "text"
      },
      "source": [
        "### Fixer la seed\n",
        "\n",
        "Afin de reproduire nos résultats obtenus à chaque fois et ainsi voir si nos modifications apportent une réelle amélioration au système de base, il faut fixer la *seed* !\n",
        "La *seed* est un nombre qui est utilisé pour l'initialisation du générateur de nombres pseudo-aléatoires.\n",
        "\n",
        "Sur Keras avec Tensorflow en backend, il faut fixer la *seed* des bibliothèques *Numpy* et *Tensorflow*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_U8sCtjVvS3E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(123) # On peut utiliser n'importe quel nombre\n",
        "\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(1234)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKxcycI7tkV_",
        "colab_type": "text"
      },
      "source": [
        "## Partie I : Classification avec le dataset Fashion-MNIST\n",
        "\n",
        "Durant la première partie, nous allons mettre en place un réseau de neurones de type MLP (Multi-Layer Perceptron) afin de résoudre une tâche de classication.\n",
        "\n",
        "Cette tâche consiste de classifier correctement des images de vêtements (pull, pantalon, jupe, sac, ...) issues du corpus Fashion-MNIST."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y28gf7P4u__A",
        "colab_type": "text"
      },
      "source": [
        "#### Téléchargement et affichage des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PtKHt3cR2Jf",
        "colab_type": "text"
      },
      "source": [
        "**Téléchargement du corpus**\n",
        "\n",
        "La première étape consiste à télécharger le dataset *Fashion-MNIST*.\n",
        "\n",
        "Le corpus *Fashion-MNIST* a été réalisé par l'équipe de recherche de Zalando proposant une alternative au corpus *MNIST* en utilisant les mêmes tailles des images et la même structure. Il est constitué d'articles vendu par Zalando.\n",
        "\n",
        "Il est donc facile de tester notre réseau entre *MNIST* et *Fashion-MNIST* en changeant 2 lignes de code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojIkefO9u0Pj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Chargement du dataset MNIST:\n",
        "#from keras.datasets import mnist\n",
        "#(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Chargement du dataset Fashion-MNIST:\n",
        "from keras.datasets import fashion_mnist\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exWrg9G8xarF",
        "colab_type": "text"
      },
      "source": [
        "**Taille du corpus**\n",
        "\n",
        "Affichage de la taille du corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdB0VtaaxaWG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Affichage du nombre d'exemples totales dans le corpus\n",
        "print('Taille du corpus total')\n",
        "print('\\t• train :', len(x_train), 'exemples')\n",
        "print('\\t• test :', len(x_test), 'exemples')\n",
        "\n",
        "# Affichage de la taille des images et des labels dans le corpus \n",
        "print('\\nTaille des données d\\'apprentissage')\n",
        "print('\\t• X_train (images) :', x_train.shape)\n",
        "print('\\t• y_train (labels) :', y_train.shape)\n",
        "\n",
        "print('\\nTaille des données de test')\n",
        "print('\\t• X_test (images) :', x_test.shape)\n",
        "print('\\t• y_test (labels) :', y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jH4cUZr0yye",
        "colab_type": "text"
      },
      "source": [
        "**Correspondance ID du label / signification**\n",
        "\n",
        "Définition de la liste *idx_to_classes* permettant à partir de la valeur du label de retrouver sa signification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8BQY_CX00B-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# liste de conversion dans le cas de MNIST\n",
        "#idx_to_classes = ['zero', 'un', 'deux', 'trois', 'quatre',\n",
        "#                  'cinq', 'six', 'sept', 'huit', 'neuf']\n",
        "\n",
        "# liste de conversion dans le cas de Fashion-MNIST\n",
        "idx_to_classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "                  'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Anlke boot']\n",
        "\n",
        "\"\"\"\n",
        "Affichage des labels des 10 premières images d'entraînement\n",
        "\"\"\"\n",
        "# exemple en affichant la correspondance des labels sur 10 premières données\n",
        "# du corpus d'apprentissage\n",
        "print(\"Affichage de la correspondance des labels :\")\n",
        "for i in range(10):\n",
        "    print('• y_train[' + str(i) + '] =', y_train[i], '->', idx_to_classes[y_train[i]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOTXwUQmRgB6",
        "colab_type": "text"
      },
      "source": [
        "**Affichage des images**\n",
        "\n",
        "On affiche maintenant quelques images issues du corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4wvbHWERgpd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(12,4))\n",
        "for i in range(14):\n",
        "    # récupération d'une image et de son label associé\n",
        "    img, target = x_train[i], y_train[i]\n",
        "    # choix de la zone d'affichage et afficage de l'image en niveau de gris\n",
        "    plt.subplot(2,7,i+1)\n",
        "    plt.imshow(img, cmap=\"gray\")\n",
        "    # ajout d'un titre à l'image    \n",
        "    plt.title('{} ({})'.format(idx_to_classes[target], target))\n",
        "    plt.axis('off')\n",
        "              \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stXSTB0HvI_k",
        "colab_type": "text"
      },
      "source": [
        "#### Prétraitements des données\n",
        "\n",
        "Dans cette section, nous allons voir quelles sont les prétraitements basique à réaliser sur les données avant de les alimenter à notre réseau de neurones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtENEQVvNBk8",
        "colab_type": "text"
      },
      "source": [
        "**Conversion en float**\n",
        "\n",
        "Afin de faciliter les différentes opérations de traitements sur les images, on va convertir les images d'apprentissage et de test en type float 32 bits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfZiAwGY7_Rd",
        "colab_type": "text"
      },
      "source": [
        "*Question* : quel est le type des images x_train et x_test pour l'instant ? \n",
        "Pour répondre, afficher le tableau de la première image dans x_train. Quel est l'intervalle de valeurs possible des pixels ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4rJGr8S9TCj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# afficher les valeurs des pixels de la première image de x_train\n",
        "img, target = x_train[0], y_train[0]\n",
        "for i in range(10):\n",
        "    print(img[i])  # ndarray d'entiers [0, 254]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlmxDnQu8xeV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Conversion des données en type float32:\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\"\"\"\n",
        "img, target = x_train[0], y_train[0]\n",
        "for i in range(10):\n",
        "    print(img[i])\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Niy-Sb6x4SbM",
        "colab_type": "text"
      },
      "source": [
        "**Encodage one-hot**\n",
        "\n",
        "Comme il a été présenté dans le cours, lors d'une tâche de classification, la sortie du réseau dispose de $n$ neurones correspondants au $n$ classes que l'on désire en sortie.\n",
        "\n",
        "Afin de calculer notre fonction de coût pour ensuite appliquer l'algorithme de descente de gradient sur notre réseau, il faut au préalable encoder les labels dans le format **one-hot** qui consiste à représenter notre label en vecteur de valeur binaire dont sa représentation n'a qu'un seul 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWeoNwAs4S7h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb_classes = 10\n",
        "\n",
        "# one-hot encoding\n",
        "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
        "\n",
        "# affichage de l'encodage des labels sur les 10 premières données\n",
        "# du corpus d'apprentissage\n",
        "print(\"Affichage de l'encodage des labels :\")\n",
        "for i in range(10):\n",
        "    print('• y_train[' + str(i) + '] =', y_train[i], '->', Y_train[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRc99IvBrcum",
        "colab_type": "text"
      },
      "source": [
        "**Normalisation des données**\n",
        "\n",
        "Dans le domaine du traitement des images, de nombreuses méthodes peuvent être utilisées, en voici deux exemples :\n",
        "\n",
        "* **rééchelonnage :** normalisation linéaire consistant à étaler la dynamique totale de l'image dans un nouvel intervalle. Dans notre cas, l'intervalle est \\[0,1\\].\n",
        "* **standardisation :** normalisation linéraire consistant à soustraire la moyenne et diviser par l'écart-type.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9oo65uaonU0",
        "colab_type": "text"
      },
      "source": [
        "*Exercice* définir la fonction rééchelonnage ou la standardisation (ou les 2), puis l'appliquer sur vos images d'apprentissage et de tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4-8xswprWTF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# définition du rééchelonnage\n",
        "def reechelonnage(X_train, X_test):\n",
        "    for i in X_train :\n",
        "      i /= 255\n",
        "    for j in X_test :\n",
        "      j /= 255\n",
        "    return X_train, X_test\n",
        "\"\"\"\n",
        "x_train, x_test = reechelonnage(x_train, x_test)\n",
        "print(x_train[5], \"\\n\\n\")\n",
        "print(x_test[5])\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-WSAfRFt3v-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# définition de la standardisation\n",
        "def standardisation(X_train, X_test):\n",
        "    m = np.mean(X_train)\n",
        "    e = np.nanstd(X_train)\n",
        "    X_train = (X_train - m) / e\n",
        "\n",
        "    X_test = (X_test - m) / e\n",
        "    return X_train, X_test\n",
        "\n",
        "x_train, x_test = standardisation(x_train, x_test)\n",
        "#print(x_train[5], \"\\n\\n\")\n",
        "#print(x_test[5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GbWG5nePTZT",
        "colab_type": "text"
      },
      "source": [
        "On applique une des normalisations définies précédemment sur nos images du corpus d'apprentissage et de tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGFYCzKvt-Jw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sans normalisation\n",
        "#X_train, X_test = x_train, x_test\n",
        "\n",
        "# normalisation des données\n",
        "X_train, X_test = standardisation(x_train, x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOQLjOdhYD7k",
        "colab_type": "text"
      },
      "source": [
        "**Applatissement des images**\n",
        "\n",
        "Comme vous l'avez vu précédemment lors du calcul de l'affichage des dimensions des données de notre corpus, les images sont représentées par une matrice 2D.\n",
        "Dans le cas d'un réseau de neurones MLP/FCNN, l'entrée du réseau est un vecteur de neurones et non une matrice 2D de neurones, il faut donc vectoriser nos données en entrée de notre réseau.\n",
        "\n",
        "Pour cela, on pourra s'aider de la fonction *reshape* afin de modifier la taille des données (pour \"applatir\" la matrice 2D en vecteur 1D)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1U-fFnSpXp8J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def img_reshape(X_train, X_test):\n",
        "    X_train = X_train.reshape((-1,28*28))\n",
        "    X_test = X_test.reshape((-1,28*28))\n",
        "    return X_train, X_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HA5M-QgOX98s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test = img_reshape(X_train, X_test)\n",
        "\n",
        "print('\\nTaille des données :')\n",
        "print('\\t• X_train :', X_train.shape)\n",
        "print('\\t• X_test :', X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUC46t-vv0i7",
        "colab_type": "text"
      },
      "source": [
        "#### Création et apprentissage d'un modèle MLP\n",
        "\n",
        "Dans cette section, nous allons nous intéresser à la définition d'un modèle MLP puis lancer l'apprentissage en utilisant la bibliothèque Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82fVwfLHUn8u",
        "colab_type": "text"
      },
      "source": [
        "**Définition du modèle**\n",
        "\n",
        "Dans la cellule suivante, on va définir un modèle qui va être entraîné sur le corpus Fashion-MNIST afin de classifier suivant les 10 classes que l'on souhaite.\n",
        "\n",
        "Tout d'abord, lire cette page de documentation de Keras qui vous explique comment définir un modèle \"Sequential\" : \n",
        "\n",
        "https://keras.io/getting-started/sequential-model-guide/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EY91NiAgeBnG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model initialization:\n",
        "model = Sequential()\n",
        "\n",
        "# Notre premier modele avec une seule couche cachée de 50 neurones.\n",
        "\n",
        "# Pour le construire, ajouter une couche Dense. \n",
        "# Attention vous devez préciser la dimension des inputs pour cette première couche :\n",
        "#model.add(keras.Input(shape=(784,)))\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        Dense(50, activation=\"relu\", name=\"layer1\", input_shape=(784,))\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Ensuite ajouter une activation de type ReLu :\n",
        "#model.add(Activation(activations.relu))\n",
        "\n",
        "# Ajouter maintenant la couche de sortie avec le bon nombre de neurones et la bonne fonction d'activation :\n",
        "model.add(Dense(10, activation=\"softmax\", name=\"output\"))\n",
        "\n",
        "# Compléter la ligne suivante en choisissant la bonne fonction de coût.\n",
        "# Voir la liste des fonctions de coût disponibles ici :\n",
        "# https://keras.io/losses/\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',  #binary_crossentropy avec Keras\n",
        "              optimizer='rmsprop', \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUH2CLsEXNpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Affichage des informations sur le réseau défini au-dessus\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnsILh0gxSHh",
        "colab_type": "text"
      },
      "source": [
        "**Mode apprentissage**\n",
        "\n",
        "Lancer l'apprentissage du modèle sur nos données. Compléter les arguments de la méthode *fit* ci-dessous"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikX4T0A5wOhU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "epochs = 10 \n",
        "\n",
        "# compléter les arguments nécessaires ci-dessous\n",
        "history = model.fit(X_train, Y_train, epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60hkki8nWtnd",
        "colab_type": "text"
      },
      "source": [
        "**Affichage de données issus de l'apprentissage**\n",
        "\n",
        "On va maintenant afficher graphiquement des informations que l'on a récupéré après l'apprentissage de notre modèle\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBSsxNUwxVKP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(11,3))\n",
        "\n",
        "# affichage de la valeur de la fonction de perte\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history.epoch,history.history['loss'])\n",
        "plt.title('loss')\n",
        "\n",
        "# affichage de la précision de notre réseau sur les données d'apprentissage\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history.epoch,history.history['accuracy'])\n",
        "plt.title('accuracy');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgzOuN7aBrgE",
        "colab_type": "text"
      },
      "source": [
        "Que pouvez-vous dire sur ces courbes ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJyGDoqFx9JS",
        "colab_type": "text"
      },
      "source": [
        "### Inférence des données de tests\n",
        "\n",
        "On va maintenant évaluer notre modèle sur des exemples non vus pendant l'apprentissage.\n",
        "Pour cela, il suffit juste de présenter nos données au modèle sans réaliser la descente de gradient, on appelle cela faire une inférence, c'est-à-dire que l'on va juste faire des prédictions sur nos données."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3B568RBxyio",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "scores = model.evaluate(X_test, Y_test, verbose=2)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFKt0xXqBy1q",
        "colab_type": "text"
      },
      "source": [
        "Ce score est-il bon ? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Znqogu5IyTTq",
        "colab_type": "text"
      },
      "source": [
        "**Affichage des erreurs de prédictions**\n",
        "\n",
        "Regardons des erreurs de prédictions faites par le modèle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdK_-sqQyLFP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_failures(predictions, trueclass=None, predictedclass=None, maxtoshow=10):\n",
        "    rounded = np.argmax(predictions, axis=1)\n",
        "    errors = rounded!=y_test\n",
        "    print('La prédiction est affichée en premier et entre parenthèses, le vrai label.')\n",
        "    ii = 0\n",
        "    plt.figure(figsize=(maxtoshow+maxtoshow/4, 1))\n",
        "    for i in range(x_test.shape[0]):\n",
        "        if ii>=maxtoshow:\n",
        "            break\n",
        "        if errors[i]:\n",
        "            if trueclass is not None and y_test[i] != trueclass:\n",
        "                continue\n",
        "            if predictedclass is not None and rounded[i] != predictedclass:\n",
        "                continue\n",
        "            plt.subplot(1, maxtoshow, ii+1)\n",
        "            plt.axis('off')\n",
        "            img = X_test[i].reshape(28, 28)\n",
        "            plt.imshow(img, cmap=\"gray\")\n",
        "            plt.title(\"{}\\n({})\".format(idx_to_classes[rounded[i]], idx_to_classes[y_test[i]]))\n",
        "            ii = ii + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0LyZSr4ycfW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict(X_test)\n",
        "\n",
        "show_failures(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CP3iJ-vHyjTG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pour afficher les erreurs du chiffre 6 uniquement :\n",
        "show_failures(predictions, trueclass=6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGPNYjNycsnk",
        "colab_type": "text"
      },
      "source": [
        "En général, dans une tâche de classification en apprentissage supervisé, on affiche la matrice de confusions pour présenter les résultats."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dmCe6DtzGvh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "print('Confusion matrix (rows: true classes; columns: predicted classes):'); print()\n",
        "cm=confusion_matrix(y_test, np.argmax(predictions, axis=1), labels=list(range(10)))\n",
        "print(cm); print()\n",
        "\n",
        "print('Classification accuracy for each class:'); print()\n",
        "acc = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "for i,j in enumerate(acc.diagonal()):\n",
        "    print(\"%d: %.4f\" % (i,j))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZgiT4qUKmrR",
        "colab_type": "text"
      },
      "source": [
        "On la lit ** par lignes** !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1NMvXesdwV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# afficage de la matrice de confusions dans un autre format\n",
        "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.hot_r)\n",
        "plt.title('Confusion matrix')\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(len(idx_to_classes))\n",
        "plt.grid(None)\n",
        "classes_name = zip(idx_to_classes, list(range(len(idx_to_classes))))\n",
        "classes_name = ['{} ({})'.format(*i) for i in classes_name]\n",
        "plt.xticks(tick_marks, classes_name, rotation=45)\n",
        "plt.yticks(tick_marks, classes_name)\n",
        "\n",
        "thresh = cm.max() / 2.\n",
        "for i in range(cm.shape[0]):\n",
        "    for j in range(cm.shape[1]):\n",
        "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\",\n",
        "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Et1oAUsrB5Ty",
        "colab_type": "text"
      },
      "source": [
        " **Playground**\n",
        "\n",
        "Vous allez essayer d'améliorer la performance de votre modèle simple.\n",
        "\n",
        "Reprendre et modifier votre MLP pour améliorer le taux de bonne \n",
        "classification ou pour voir l'influence des paramètres : \n",
        "\n",
        "*  en ajoutant une ou plusieurs couches cachées,\n",
        "\n",
        "* en augmentant le nombre de neurones,\n",
        "\n",
        "* en changeant la fonction d'activation des couches cachées,\n",
        "\n",
        "* en rendant plus robuste votre MLP avec des couches de \"dropout\" et en testant différents taux de dropout\n",
        "\n",
        "* Consulter la documentation de Keras https://keras.io/, par exemple sur les couches Dense, Activation et Dropout : https://keras.io/layers/core/\n",
        "\n",
        "* Vous pouvez aussi jouer sur le nombre d'epochs d'apprentissage, la taille du mini-batch, changer l'optimizer, etc. Voir https://keras.io/optimizers/\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qB7N3_2UwQL",
        "colab_type": "text"
      },
      "source": [
        "### (Optionnel) Codage de la fonction de coût (_loss function_)\n",
        "Lors de l'utilisation de la méthode **_compile_** de votre modèle précèdent, vous avez spécifié une fonction de perte pour le calcul des gradients qui était la __*categorical\\_crossentropy*__ définis par la fonction suivante :\n",
        "\n",
        "$$\n",
        "- \\sum_{i=0}^{N} y_i \\log(\\hat{y}_i)\n",
        "$$\n",
        "*Pas diviser par 1/N !!*\n",
        "\n",
        "avec:\n",
        "* $N$ : le nombre totale de classes\n",
        "* $y$ : la sortie du réseau voulue\n",
        "* $\\hat{y}$: la sortie du réseau estimée\n",
        "\n",
        "**Exercice :** dans la cellule suivante, essayer de recoder la fonction de coût *categorical_crossentropy* et l'utiliser dans votre modèle précédent pour voir si elle fonctionne\n",
        "\n",
        "Aide :\n",
        "* Toutes les fonctions à utiliser sont dans le backend de Keras. Exemple : pour faire une somme, utiliser K.sum()\n",
        "* Attention, il faut limiter les valeurs du log dans [1e-7, 1.0-1e-7] par exemple. Pour cela utiliser K.clip()\n",
        "* Les dimensions de la vérité terrain et des prédictions est (batch_size, nb_classes). La fonction s'applique sur chaque élément du batch séparément et renvoie un tenseur de dimension (batch_size, ).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kquVW5TsUxfJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def categorical_crossentropy(y_true, y_pred):\n",
        "    # Permet d'éviter les problèmes avec le log quand y_pred tend vers 0 \n",
        "    # A faire : limiter les valeurs de y_pred :\n",
        "    y_pred = K.clip(y_pred, 1e-7, 1.0-1e-7)\n",
        "    batch_size, nb_classes = y_pred.shape[0], y_pred.shape[1]\n",
        "    # Calcul de l'entropie croisée\n",
        "    cost = [0]*batch_size\n",
        "    for i in range(batch_size):\n",
        "      cost[i] = - K.sum(y_true[i] * K.log(y_pred[i]))  #1/nb_classes *\n",
        "    return cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7-DZQ0VW4dL",
        "colab_type": "text"
      },
      "source": [
        "**Test :** comparaison avec la fonction de coût définie dans Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LKiGS2XW0Wf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_true = K.variable(value=np.array([[1, 0, 0],\n",
        "                                    [0, 0, 1]]))\n",
        "\n",
        "y_pred = K.variable(value=np.array([[0, 0.5, 0.5],\n",
        "                                    [0.8, 0.1, 0.1]]))\n",
        "\n",
        "# remarque : toutes les opérations faisant appel au backend K sont symboliques. \n",
        "# Il faut donc utiliser K.eval() pour exécuter les opérations sur des tenseurs numériques\n",
        "\n",
        "loss1 = K.eval(K.categorical_crossentropy(y_true, y_pred, from_logits=False))\n",
        "loss2 = K.eval(categorical_crossentropy(y_true, y_pred))\n",
        "print(loss1)\n",
        "print(loss2)\n",
        "\n",
        "# vous devez obtenir True en sortie de ce test \n",
        "print(\"Test de validité : \" + str(K.eval(K.all(loss1 == loss2))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKOqfVpUjiHD",
        "colab_type": "text"
      },
      "source": [
        "Remplacer à présent la valeur de l'option _loss_ de la méthode _compile_ avec votre fonction de coût.\n",
        "Vérifier que l'apprentissage se passe correctement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46JJwAzJU7qI",
        "colab_type": "text"
      },
      "source": [
        "## Partie II : Régression sur le dataset Boston Housing Prices\n",
        "\n",
        "Pour cette deuxième partie, nous allons nous intéresser à une autre tâche de l'apprentissage machine qui est la régression de données, c'est-à-dire que l'on va estimer la valeur d'une variable par rapport à une ou plusieurs autres variables.\n",
        "\n",
        "Pour résoudre cette tâche, nous allons utiliser le corpus *Boston Housing Prices* qui consiste à partir de 13 attributs de maisons situées à différents endroits dans la banlieue de Boston à la fin des années 1970, de trouver les valeurs médianes du prix des maisons à un emplacement de Boston (en k$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCG0FGHF2z_L",
        "colab_type": "text"
      },
      "source": [
        "#### Téléchargement et affichage des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkTR_5fJ4BUz",
        "colab_type": "text"
      },
      "source": [
        "**Téléchargement du corpus**\n",
        "\n",
        "Comme dans la première partie, on télécharge le dataset *Boston Housing Prices* dont l'interface de chargement des données est présente dans la bibliothèque Keras.\n",
        "\n",
        "Ce corpus a été réalisé par Harrison and Rubinfeld dans les années 70."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRX59X7KU8B3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import boston_housing\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upHFM6_f36r8",
        "colab_type": "text"
      },
      "source": [
        "**Taille du corpus**\n",
        "\n",
        "Affichage de la taille du corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iK0tFxxY35rS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Affichage du nombre d'exemples totales dans le corpus\n",
        "print('Taille du corpus total')\n",
        "print('\\t• train :', len(x_train), 'exemples')\n",
        "print('\\t• test :', len(x_test), 'exemples')\n",
        "\n",
        "# Affichage de la taille des attributs du corpus \n",
        "print('\\nTaille des données d\\'apprentissage')\n",
        "print('\\t• X_train (attributs) :', x_train.shape)\n",
        "print('\\t• y_train (valeur à estimer) :', y_train.shape)\n",
        "\n",
        "print('\\nTaille des données de test')\n",
        "print('\\t• X_test (attributs) :', x_test.shape)\n",
        "print('\\t• y_test (valeur à estimer) :', y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umOjEKZD5lnA",
        "colab_type": "text"
      },
      "source": [
        "**Informations sur le corpus**\n",
        "\n",
        "Comme vous avez pu voir lors de la visualisation dans la cellule précédente, chaque élément du corpus dispose de 13 attributs qui sont les suivants :\n",
        "\n",
        "- **CRIM :** taux de criminalité par habitant par ville ;\n",
        "- **ZN :** proportion de terrains résidentiels zonés pour les lots de plus de 25 000 pi.ca. (pied carré, système de mesure américain) ;\n",
        "- **INDUS :** proportion d'acres d'activité non commerciale par ville (l'acre est une ancienne unité de mesure de superficie anglaise) ;\n",
        "- **CHAS :** variable à 1 si le terrain est délimité par la rivière Charles, 0 sinon ;\n",
        "- **NOX :** concentration de monoxyde d'azote ;\n",
        "- **RM :** nombre moyen de pièces par logement ;\n",
        "- **AGE :** proportion de logements occupés par leur propriétaire construits avant 1940 ;\n",
        "- **DIS :**  les distances pondérées jusqu'à cinq centres d'emploi de Boston ;\n",
        "- **RAD :** indice d'accessibilité aux autoroutes radiales ;\n",
        "- **TAX :** taux d'imposition foncière ;\n",
        "- **PTRATIO :** ratio élèves/maître par ville ;\n",
        "- **B :** $1000\\cdot (Bk - 0.63)^2$ où $Bk$ est une densité d'habitants par ville ;\n",
        "- **LSTAT :** statut inférieur de la population.\n",
        "\n",
        "Concernant la valeur qu'on essaie de trouver à partir des attributs précédent est la valeur médiane des maisons occupées par leur propriétaire en milliers de dollars **MEDV**).\n",
        "On recherche donc les corrélations des attributs précédents sur le prix médian des maisons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eT5Dhqc6orZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# liste de tous les attributs du corpus\n",
        "attributes = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE',\n",
        "              'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
        "\n",
        "# affichage des données\n",
        "df = pd.DataFrame(np.concatenate((x_train, y_train[:,None]), axis=1), columns=attributes)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxo_ieUPH3L_",
        "colab_type": "text"
      },
      "source": [
        "À partir des 13 premiers attributs du tableau précédent, on cherche à prédire la dernière colonne.\n",
        "\n",
        "Les valeurs de la dernière colonne du tableau sont les prix des maisons en milliers de dollars (Vous pouvez remarquer les prix des maisons dans les années 70 !) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95mMgrRCIX2h",
        "colab_type": "text"
      },
      "source": [
        "#### Prétraitements des données\n",
        "\n",
        "On va maintenant prétraiter les données comme dans le cas précédent sur le problème de classification.\n",
        "\n",
        "On va d'abord caster en float puis normaliser les données en utilisant la standardisation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOTrLimwLti7",
        "colab_type": "text"
      },
      "source": [
        "**Conversion en float**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n--Y0tNvLszq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Conversion des données en type float\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PeuKfHoL9Nr",
        "colab_type": "text"
      },
      "source": [
        "**Normalisation des données**\n",
        "\n",
        "On peut appliquer le même type de normalisation que les images sur nos données d'apprentissage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYmoK-blWo7G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# définition du rééchelonnage\n",
        "def reechelonnage(X_train, X_test):\n",
        "    X_train /= X_train.max(axis=0)\n",
        "    X_test /= X_test.max(axis=0)\n",
        "    return X_train, X_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsoGkGN7CXUL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# définition de la standardisation\n",
        "def standardisation(X_train, X_test):\n",
        "    mean = np.mean(X_train, axis=0)\n",
        "    var = np.var(X_train, axis=0)\n",
        "    return ((X_train - mean) / var), ((X_test - mean) / var)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDiD8yKGMJeN",
        "colab_type": "text"
      },
      "source": [
        "On applique la standardisation sur toutes nos données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7k52l0lVMO0G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test = standardisation(x_train, x_test)\n",
        "Y_train, Y_test = y_train, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA2QjwrCNxpk",
        "colab_type": "text"
      },
      "source": [
        "On peut de nouveau afficher les données du corpus d'apprentissage pour voir qu'elles sont normalisées "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8ZK5zk5M6k5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame(np.concatenate((X_train, Y_train[:,None]), axis=1), columns=attributes)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfenuRt0OVbM",
        "colab_type": "text"
      },
      "source": [
        "#### Création et apprentissage d'un modèle MLP\n",
        "\n",
        "Dans cette section, nous allons définir un nouveau modèle de réseau de neurones pour répondre à la tâche de régression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5UjBLHNOYGp",
        "colab_type": "text"
      },
      "source": [
        "**Définition du modèle**\n",
        "\n",
        "Dans la cellule suivante, on va définir notre premier modèle de régression linéaire qui va être entraîner sur notre corpus *Boston Housing Price* afin de prédire le prix médian des maisons suivant 13 attributs.\n",
        "\n",
        "La régression linéaire est représenté par des entrées directement connectées au neurone de sortie. L'idée est d'établir une relation linéaire entre une variable (celle en sortie) par rapport à d'autres variables qui sont en entrée de notre réseau."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37fndqTCNAG8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "# régression linéaire\n",
        "model.add(Dense(1, input_dim=13, activation='relu'))\n",
        "\n",
        "# Compiler le modèle en choissisant la bonne fonction de coût et un optimizer\n",
        "model.compile(loss='mse', # Mean Squared error\n",
        "              optimizer='sgd',\n",
        "              metrics=['mae'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPi4TWfCOxvw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Affichage des informations sur le réseau défini au-dessus :\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBe_l7N2O1PW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "early_stop = keras.callbacks.EarlyStopping(monitor='loss', patience=40)\n",
        "\n",
        "# compléter les arguments de *fit* pour faire un apprentissage sur:\n",
        "#  200 epochs,\n",
        "#  avec mini-batches de 32 exemples\n",
        "\n",
        "history = model.fit(X_train, \n",
        "                    Y_train,\n",
        "                    epochs=200, \n",
        "                    batch_size=32,\n",
        "                    verbose=2,\n",
        "                    callbacks=[early_stop])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_57l9bfpR4rX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(8,3))\n",
        "\n",
        "# affichage de la valeur de la fonction de perte\n",
        "plt.plot(history.epoch,history.history['loss'])\n",
        "plt.title('loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8q09AeiZi2g",
        "colab_type": "text"
      },
      "source": [
        "### Inférence des données de tests\n",
        "\n",
        "Évaluons notre modèle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0FeChTJZRbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[loss, mae] = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(\"Testing set Mean Abs Error: ${:.2f}\".format(mae*1000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NryPpii3Su-",
        "colab_type": "text"
      },
      "source": [
        "Affichons les valeurs prédites sur les données de test pour les comparer aux valeurs réelles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnBfFdjVA6ke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Récupération des prédictions\n",
        "predictions = model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJAFbfasZiTb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "plt.plot(predictions)\n",
        "plt.plot(y_test)\n",
        "plt.legend(['predictions', 'labels'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfWhqKWQCbZF",
        "colab_type": "text"
      },
      "source": [
        "### Playground\n",
        "\n",
        "Modifier le réseau pour avoir deux couches cachées à 64 neurones et analyser les résultats"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Mru93Z1i-gE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "# régression linéaire\n",
        "# Couche d'entrée (13) + Première couche cachées (64)\n",
        "model.add(Dense(64, input_dim=13, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "\n",
        "# Compiler le modèle en choissisant la bonne fonction de coût et un optimizer\n",
        "model.compile(loss='mse', # Mean Squared error\n",
        "              optimizer='sgd',\n",
        "              metrics=['mae'])\n",
        "\n",
        "# fin de votre code\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKrhUqGkDxxo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = keras.optimizers.RMSprop(0.001)\n",
        "\n",
        "model.compile(loss='mse', \n",
        "              optimizer=optimizer,\n",
        "              metrics=['mae'])\n",
        "\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor='loss', patience=20)\n",
        "\n",
        "history = model.fit(X_train, \n",
        "                    Y_train,\n",
        "                    epochs=500, \n",
        "                    batch_size=32,\n",
        "                    verbose=2,\n",
        "                    validation_split=0.2,\n",
        "                    callbacks=[early_stop])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6WgpPxsHrCT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Mean Abs Error [1000$]')\n",
        "plt.plot(history.epoch, np.array(history.history['mae']), # mean_absolute_error\n",
        "       label='Train Loss')\n",
        "plt.plot(history.epoch, np.array(history.history['val_mae']), # val_mean_absolute_error\n",
        "         label = 'Val loss')\n",
        "plt.legend()\n",
        "plt.ylim([0,5])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY7bfJSvDhxp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(8,3))\n",
        "\n",
        "# affichage de la valeur de la fonction de perte\n",
        "plt.plot(history.epoch,history.history['loss'])\n",
        "plt.title('loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOyaoEHpD86J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[_, mae] = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(\"Testing set Mean Abs Error: ${:.2f}\".format(mae*1000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qt2fvgi6Cb4G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Faire des prédictions sur X_test :\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Affichage :\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.plot(predictions)\n",
        "plt.plot(y_test)\n",
        "plt.legend(['predictions', 'labels'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}